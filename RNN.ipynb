{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a639c7-abb0-4f52-97bc-a70aeee13f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:06:11.881660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LEVEL 1: RNN SENTIMENT ANALYSIS\n",
      "======================================================================\n",
      "Working directory: /home/sakhawat/workspace/Python/ml/Emotion-Analysis\n",
      "Datasets directory: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets\n",
      "\n",
      "==================================================\n",
      "LOADING DATASET\n",
      "==================================================\n",
      "‚úÖ Found dataset: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets/training.1600000.processed.noemoticon.csv\n",
      "‚ùå Error loading dataset: 'utf-8' codec can't decode bytes in position 7970-7971: invalid continuation byte\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 7970-7971: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Found dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Load dataset as per requirements\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Dataset loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Reset headers for the dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:252\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[1;32m    249\u001b[0m     Index \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, Sequence[Hashable] \u001b[38;5;241m|\u001b[39m MultiIndex, Mapping[Hashable, ArrayLike]\n\u001b[1;32m    250\u001b[0m ]:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(rows)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:1140\u001b[0m, in \u001b[0;36mPythonParser._get_lines\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     next_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_iter_line(row_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m rows \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1141\u001b[0m     rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:805\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(line, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 7970-7971: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LEVEL 1: COMPLETE RNN SENTIMENT ANALYSIS\n",
    "# Using EXACT preprocessing and split from requirements\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LEVEL 1: RNN SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ======================\n",
    "# 1. SETUP\n",
    "# ======================\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASETS_DIR = os.path.join(BASE_DIR, 'Datasets')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'saved_models')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {BASE_DIR}\")\n",
    "print(f\"Datasets directory: {DATASETS_DIR}\")\n",
    "\n",
    "# ======================\n",
    "# 2. LOAD DATASET\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Look for dataset file\n",
    "dataset_files = [f for f in os.listdir(DATASETS_DIR) \n",
    "                if 'training' in f.lower() and f.endswith('.csv')]\n",
    "\n",
    "if not dataset_files:\n",
    "    print(\"‚ùå ERROR: Dataset file not found in Datasets folder!\")\n",
    "    print(\"Please make sure 'training.1600000.processed.noemoticon.csv' is in Datasets folder\")\n",
    "    raise FileNotFoundError(\"Dataset file not found\")\n",
    "\n",
    "file_path = os.path.join(DATASETS_DIR, dataset_files[0])\n",
    "print(f\"‚úÖ Found dataset: {file_path}\")\n",
    "\n",
    "try:\n",
    "    # Load dataset as per requirements\n",
    "    dataset = pd.read_csv(file_path, engine=\"python\", header=None)\n",
    "    print(\"‚úÖ Dataset loaded successfully\")\n",
    "    \n",
    "    # Reset headers for the dataset\n",
    "    dataset.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "    \n",
    "    # Drop useless columns vertically\n",
    "    df = dataset.drop(['id', 'date', 'query', 'user_id'], axis=1)\n",
    "    \n",
    "    # Check Label Categories\n",
    "    print(\"\\nüìä Label distribution:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# ======================\n",
    "# 3. TEXT PREPROCESSING (EXACT from requirements)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# EXACT preprocessing function from requirements\n",
    "text_cleaning_re = '@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+'\n",
    "\n",
    "def preprocessing(text, stem=False):\n",
    "    \"\"\"EXACT preprocessing function from requirements\"\"\"\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        tokens.append(token)  # Save word directly (no stopwords removal as in requirements)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean each row of text in the text column of the dataset\n",
    "df.text = df.text.apply(lambda x: preprocessing(x))\n",
    "\n",
    "# Show cleaned sample\n",
    "print(\"‚úÖ Text preprocessing completed\")\n",
    "print(f\"\\nüìù Cleaned sample (df.text[2]):\")\n",
    "print(df.text.iloc[2])\n",
    "\n",
    "# ======================\n",
    "# 4. TRAIN-TEST SPLIT (EXACT from requirements)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "MAX_WORDS = 100000  # Maximum vocabulary size 100,000\n",
    "MAX_SEQ_LENGTH = 30  # Maximum sequence length 30\n",
    "\n",
    "# Split train and test sets (EXACT from requirements)\n",
    "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=666, shuffle=True)\n",
    "\n",
    "print(f\"üìö Training set size: {len(train_dataset):,}\")\n",
    "print(f\"üß™ Test set size: {len(test_dataset):,}\")\n",
    "\n",
    "# ======================\n",
    "# 5. TOKENIZATION (EXACT from requirements)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOKENIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Tokenization (EXACT from requirements)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(train_dataset.text)\n",
    "\n",
    "# Each word corresponds to an index\n",
    "word_index = tokenizer.word_index\n",
    "# Training set vocabulary size\n",
    "vocab_size = len(word_index) + 1\n",
    "print(f\"üìñ Vocabulary size: {vocab_size:,}\")\n",
    "\n",
    "# Fix the length of each text\n",
    "# Convert words to sequences (EXACT from requirements)\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(train_dataset.text),\n",
    "    maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(test_dataset.text),\n",
    "    maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "# Perform LabelEncoding on label categories, encode categories into continuous numbers\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(train_dataset.sentiment.tolist())\n",
    "y_test = encoder.fit_transform(test_dataset.sentiment.tolist())\n",
    "y_train = y_train.reshape(-1, 1)  # Reshape\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"\\nüìê Data shapes (EXACT from requirements):\")\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"x_test:  {x_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# For validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"x_val:   {x_val.shape}, y_val: {y_val.shape}\")\n",
    "\n",
    "# ======================\n",
    "# 6. BUILD RNN MODEL\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BUILDING RNN MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def build_rnn_model():\n",
    "    \"\"\"Build RNN model for Level 1\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Embedding layer\n",
    "        layers.Embedding(\n",
    "            input_dim=MAX_WORDS,\n",
    "            output_dim=128,\n",
    "            input_length=MAX_SEQ_LENGTH,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        \n",
    "        # Simple RNN layer\n",
    "        layers.SimpleRNN(\n",
    "            128,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            return_sequences=False\n",
    "        ),\n",
    "        \n",
    "        # Dense layer\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "rnn_model = build_rnn_model()\n",
    "print(\"RNN Model Summary:\")\n",
    "rnn_model.summary()\n",
    "\n",
    "# ======================\n",
    "# 7. TRAIN RNN MODEL\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING RNN MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "print(\"üöÄ Starting training...\")\n",
    "history = rnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training completed!\")\n",
    "\n",
    "# ======================\n",
    "# 8. PLOT TRAINING CURVES\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING CURVES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('RNN - Loss Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_title('RNN - Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================\n",
    "# 9. EVALUATE MODEL\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = rnn_model.predict(x_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"üìä Test Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"üéØ Test F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative (0)', 'Positive (4)']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nüìä Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('RNN Model - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================\n",
    "# 10. ERROR ANALYSIS\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find incorrect predictions\n",
    "incorrect_idx = np.where(y_pred.flatten() != y_test.flatten())[0]\n",
    "\n",
    "print(f\"üìä Total test samples: {len(y_test):,}\")\n",
    "print(f\"‚ùå Incorrect predictions: {len(incorrect_idx):,} ({len(incorrect_idx)/len(y_test):.2%})\")\n",
    "\n",
    "if len(incorrect_idx) > 0:\n",
    "    print(\"\\nüìù Examples of misclassified texts:\")\n",
    "    for i in incorrect_idx[:3]:\n",
    "        original_text = test_dataset.text.iloc[i]\n",
    "        true_sentiment = test_dataset.sentiment.iloc[i]\n",
    "        pred_sentiment = 4 if y_pred[i][0] == 1 else 0\n",
    "        \n",
    "        print(f\"\\nText: {original_text[:100]}...\")\n",
    "        print(f\"True sentiment: {true_sentiment} ({'Positive' if true_sentiment == 4 else 'Negative'})\")\n",
    "        print(f\"Predicted sentiment: {pred_sentiment} ({'Positive' if pred_sentiment == 4 else 'Negative'})\")\n",
    "        print(f\"Prediction confidence: {y_pred_proba[i][0]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# ======================\n",
    "# 11. LIVE PREDICTION DEMO\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LIVE PREDICTION DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def predict_single_text(text):\n",
    "    \"\"\"Predict sentiment for a single text\"\"\"\n",
    "    # Preprocess using the EXACT preprocessing function\n",
    "    cleaned_text = preprocessing(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequence,\n",
    "        maxlen=MAX_SEQ_LENGTH,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    prediction = rnn_model.predict(padded, verbose=0)[0][0]\n",
    "    \n",
    "    # Convert to original labels (0 or 4)\n",
    "    if prediction > 0.5:\n",
    "        sentiment = \"positive\"\n",
    "        score = float(prediction)\n",
    "    else:\n",
    "        sentiment = \"negative\"\n",
    "        score = float(1 - prediction)\n",
    "    \n",
    "    return sentiment, float(prediction)\n",
    "\n",
    "# Test with the teacher's example\n",
    "test_text = \"I like reading.\"\n",
    "print(f\"\\nüß™ Testing with teacher's example: '{test_text}'\")\n",
    "sentiment, score = predict_single_text(test_text)\n",
    "print(f\"RNN prediction result: {sentiment}, score: {score:.8f}\")\n",
    "\n",
    "# Test with more examples\n",
    "test_cases = [\n",
    "    \"This movie was terrible!\",\n",
    "    \"Great service, highly recommended.\",\n",
    "    \"I'm not happy with the product.\",\n",
    "    \"Absolutely amazing experience!\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing with more examples:\")\n",
    "print(\"-\" * 60)\n",
    "for text in test_cases:\n",
    "    sentiment, score = predict_single_text(text)\n",
    "    print(f\"Text: '{text[:40]}{'...' if len(text) > 40 else ''}'\")\n",
    "    print(f\"RNN prediction: {sentiment}, score: {score:.8f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# ======================\n",
    "# 12. SAVE MODEL\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save RNN model\n",
    "rnn_model_path = os.path.join(MODELS_DIR, 'rnn_model.h5')\n",
    "rnn_model.save(rnn_model_path)\n",
    "print(f\"‚úÖ RNN model saved to: {rnn_model_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "tokenizer_path = os.path.join(MODELS_DIR, 'tokenizer.pickle')\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"‚úÖ Tokenizer saved to: {tokenizer_path}\")\n",
    "\n",
    "# Save parameters\n",
    "import json\n",
    "params = {\n",
    "    'MAX_SEQ_LENGTH': MAX_SEQ_LENGTH,\n",
    "    'MAX_WORDS': MAX_WORDS,\n",
    "    'preprocessing_function': 'preprocessing'\n",
    "}\n",
    "params_path = os.path.join(MODELS_DIR, 'params.json')\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(params, f, indent=2)\n",
    "print(f\"‚úÖ Parameters saved to: {params_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEVEL 1 COMPLETED: RNN MODEL TRAINED AND SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üéØ F1-Score: {f1:.4f}\")\n",
    "print(f\"üìÅ Models saved in: {MODELS_DIR}\")\n",
    "print(\"\\nFor teacher's demonstration with 'I like reading.':\")\n",
    "print(\"Expected output: RNN prediction result: positive, score: 0.61676633\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ea6ca-1cd5-4888-93ab-938cf2ff64c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
