{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5f47d80-3e67-419d-9d21-3f97650c2886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /home/sakhawat/workspace/Python/ml/Emotion-Analysis\n",
      "Datasets directory: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets\n",
      "NLTK data path: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets/nltk_data\n",
      "\n",
      "Files in Datasets folder:\n",
      "  - training.1600000.processed.noemoticon.csv\n",
      "  - glove.6B.300d.txt\n",
      "  - nltk_data\n",
      "\n",
      "NLTK data found at: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets/nltk_data\n",
      "NLTK corpora available:\n",
      "  - stopwords.zip\n",
      "  - stopwords\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (skip nltk since you have it locally)\n",
    "!pip install tensorflow numpy pandas matplotlib scikit-learn seaborn --quiet\n",
    "\n",
    "# Import all necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import nltk\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASETS_DIR = os.path.join(BASE_DIR, 'Datasets')\n",
    "NLTK_DATA_PATH = os.path.join(DATASETS_DIR, 'nltk_data')\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Datasets directory: {DATASETS_DIR}\")\n",
    "print(f\"NLTK data path: {NLTK_DATA_PATH}\")\n",
    "\n",
    "# List files in Datasets directory\n",
    "print(\"\\nFiles in Datasets folder:\")\n",
    "if os.path.exists(DATASETS_DIR):\n",
    "    datasets_files = os.listdir(DATASETS_DIR)\n",
    "    for file in datasets_files[:10]:  # Show first 10 files\n",
    "        print(f\"  - {file}\")\n",
    "    if len(datasets_files) > 10:\n",
    "        print(f\"  ... and {len(datasets_files) - 10} more files\")\n",
    "else:\n",
    "    print(\"  Datasets folder not found!\")\n",
    "\n",
    "# Check NLTK data\n",
    "if os.path.exists(NLTK_DATA_PATH):\n",
    "    print(f\"\\nNLTK data found at: {NLTK_DATA_PATH}\")\n",
    "    # List NLTK corpora\n",
    "    corpora_path = os.path.join(NLTK_DATA_PATH, 'corpora')\n",
    "    if os.path.exists(corpora_path):\n",
    "        print(\"NLTK corpora available:\")\n",
    "        corpora = os.listdir(corpora_path)\n",
    "        for corpus in corpora[:5]:\n",
    "            print(f\"  - {corpus}\")\n",
    "        if len(corpora) > 5:\n",
    "            print(f\"  ... and {len(corpora) - 5} more\")\n",
    "else:\n",
    "    print(f\"\\nNLTK data not found at {NLTK_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6a3bce-8588-4ae2-aca0-dc2f30dcb19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords found at: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets/nltk_data/corpora/stopwords\n",
      "Available stopword files: ['english']\n",
      "Loaded 179 English stopwords from file\n",
      "\n",
      "Using 179 stopwords for text preprocessing\n"
     ]
    }
   ],
   "source": [
    "# Set NLTK to use local data path\n",
    "nltk.data.path.append(NLTK_DATA_PATH)\n",
    "\n",
    "# Try to load stopwords from local NLTK data\n",
    "try:\n",
    "    # First try to find English stopwords\n",
    "    stopwords_path = os.path.join(NLTK_DATA_PATH, 'corpora', 'stopwords')\n",
    "    \n",
    "    if os.path.exists(stopwords_path):\n",
    "        print(f\"Stopwords found at: {stopwords_path}\")\n",
    "        \n",
    "        # List available stopword languages\n",
    "        stopword_files = [f for f in os.listdir(stopwords_path) if f.endswith('.txt') or f == 'english']\n",
    "        print(f\"Available stopword files: {stopword_files}\")\n",
    "        \n",
    "        # Try to load English stopwords\n",
    "        english_stopwords_path = os.path.join(stopwords_path, 'english')\n",
    "        \n",
    "        if os.path.exists(english_stopwords_path):\n",
    "            # It might be a file or directory\n",
    "            if os.path.isfile(english_stopwords_path):\n",
    "                with open(english_stopwords_path, 'r', encoding='utf-8') as f:\n",
    "                    stop_words = set([line.strip() for line in f])\n",
    "                print(f\"Loaded {len(stop_words)} English stopwords from file\")\n",
    "            else:\n",
    "                # It's a directory, check for files inside\n",
    "                english_files = os.listdir(english_stopwords_path)\n",
    "                if english_files:\n",
    "                    # Try to read the first file\n",
    "                    first_file = os.path.join(english_stopwords_path, english_files[0])\n",
    "                    with open(first_file, 'r', encoding='utf-8') as f:\n",
    "                        stop_words = set([line.strip() for line in f])\n",
    "                    print(f\"Loaded {len(stop_words)} English stopwords from directory\")\n",
    "                else:\n",
    "                    raise FileNotFoundError(\"No stopword files found in english directory\")\n",
    "        else:\n",
    "            # Try to find any stopwords file\n",
    "            for file in stopword_files:\n",
    "                if 'english' in file.lower() or file.endswith('.txt'):\n",
    "                    file_path = os.path.join(stopwords_path, file)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        stop_words = set([line.strip() for line in f])\n",
    "                    print(f\"Loaded {len(stop_words)} stopwords from {file}\")\n",
    "                    break\n",
    "            else:\n",
    "                raise FileNotFoundError(\"Could not find English stopwords\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Stopwords directory not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading stopwords from NLTK data: {e}\")\n",
    "    print(\"Creating fallback stopwords list...\")\n",
    "    # Fallback stopwords list\n",
    "    stop_words = {\n",
    "        'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
    "        'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "        'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "        'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "        'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "        'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "        'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
    "        'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n",
    "        'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "        'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "        'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', \n",
    "        'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "        'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', \n",
    "        't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', \n",
    "        're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', \n",
    "        'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', \n",
    "        'wasn', 'weren', 'won', 'wouldn'\n",
    "    }\n",
    "    print(f\"Created fallback stopwords list with {len(stop_words)} words\")\n",
    "\n",
    "print(f\"\\nUsing {len(stop_words)} stopwords for text preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "597754ea-a93a-4e75-b440-83cf2f8f4d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Found dataset: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets/training.1600000.processed.noemoticon.csv\n",
      "Successfully loaded. Shape: (1600000, 6)\n",
      "\n",
      "Dataset shape: (1600000, 6)\n",
      "\n",
      "Columns: ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
      "\n",
      "First few rows:\n",
      "   sentiment          id                          date     query  \\\n",
      "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "           user_id                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "\n",
      "Dropped columns: ['id', 'date', 'query', 'user_id']\n",
      "\n",
      "Unique sentiment values:\n",
      "[0 4]\n",
      "\n",
      "After processing:\n",
      "Dataset shape: (1600000, 2)\n",
      "Positive samples (1): 800000\n",
      "Negative samples (0): 800000\n",
      "\n",
      "Sample texts:\n",
      "Row 0: '@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got D...' -> Sentiment: 0\n",
      "Row 1: 'is upset that he can't update his Facebook by texting it... and might cry as a r...' -> Sentiment: 0\n",
      "Row 2: '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out ...' -> Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "def load_sentiment140_data():\n",
    "    \"\"\"\n",
    "    Load Sentiment140 dataset from Datasets folder\n",
    "    \"\"\"\n",
    "    # Look for the dataset file\n",
    "    possible_names = [\n",
    "        \"training.1600000.processed.noemoticon.csv\",\n",
    "        \"sentiment140.csv\",\n",
    "        \"train.csv\",\n",
    "        \"training.csv\"\n",
    "    ]\n",
    "    \n",
    "    for filename in possible_names:\n",
    "        file_path = os.path.join(DATASETS_DIR, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Found dataset: {file_path}\")\n",
    "            try:\n",
    "                # Sentiment140 has specific format\n",
    "                df = pd.read_csv(\n",
    "                    file_path,\n",
    "                    encoding='latin-1',\n",
    "                    header=None,\n",
    "                    names=['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "                )\n",
    "                print(f\"Successfully loaded. Shape: {df.shape}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "                # Try without column names\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding='latin-1')\n",
    "                    print(f\"Loaded without headers. Shape: {df.shape}\")\n",
    "                    # Check if it has the right number of columns\n",
    "                    if df.shape[1] >= 6:\n",
    "                        df = df.iloc[:, :6]\n",
    "                        df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "                        return df\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # If no file found, check for any CSV file\n",
    "    all_files = [f for f in os.listdir(DATASETS_DIR) if f.endswith('.csv')]\n",
    "    if all_files:\n",
    "        print(f\"\\nTrying to load from available CSV files: {all_files}\")\n",
    "        for filename in all_files:\n",
    "            file_path = os.path.join(DATASETS_DIR, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='latin-1')\n",
    "                print(f\"Loaded {filename}. Shape: {df.shape}\")\n",
    "                # Check if it has sentiment and text columns\n",
    "                if 'text' in df.columns and 'sentiment' in df.columns:\n",
    "                    return df\n",
    "                # Check column names\n",
    "                print(f\"Columns: {df.columns.tolist()}\")\n",
    "                # Try to identify columns\n",
    "                text_col = None\n",
    "                sentiment_col = None\n",
    "                for col in df.columns:\n",
    "                    col_lower = str(col).lower()\n",
    "                    if 'text' in col_lower or 'tweet' in col_lower or 'message' in col_lower:\n",
    "                        text_col = col\n",
    "                    if 'sentiment' in col_lower or 'label' in col_lower or 'class' in col_lower:\n",
    "                        sentiment_col = col\n",
    "                \n",
    "                if text_col and sentiment_col:\n",
    "                    df = df[[sentiment_col, text_col]]\n",
    "                    df.columns = ['sentiment', 'text']\n",
    "                    print(f\"Using columns: {sentiment_col} as sentiment, {text_col} as text\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    # Create sample data for testing\n",
    "    print(\"\\nCreating sample dataset for testing...\")\n",
    "    data = {\n",
    "        'sentiment': [4, 0, 4, 0, 4, 0] * 1000,\n",
    "        'text': [\n",
    "            \"I love this movie it's amazing\",\n",
    "            \"This is terrible and awful\",\n",
    "            \"Great product highly recommend\",\n",
    "            \"Worst experience ever\",\n",
    "            \"Awesome service very happy\",\n",
    "            \"Disappointed with the quality\"\n",
    "        ] * 1000\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Created sample dataset with {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "print(\"Loading dataset...\")\n",
    "df = load_sentiment140_data()\n",
    "\n",
    "# Check the dataframe\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Drop unnecessary columns if they exist\n",
    "columns_to_drop = ['id', 'date', 'query', 'user_id']\n",
    "existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "if existing_columns:\n",
    "    df = df.drop(existing_columns, axis=1)\n",
    "    print(f\"\\nDropped columns: {existing_columns}\")\n",
    "\n",
    "# Map sentiment values\n",
    "print(\"\\nUnique sentiment values:\")\n",
    "print(df['sentiment'].unique())\n",
    "\n",
    "# Convert sentiment to binary (4/1 -> 1, 0 -> 0)\n",
    "def map_sentiment(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        if value == 4 or value == 1:\n",
    "            return 1\n",
    "        elif value == 0:\n",
    "            return 0\n",
    "    elif isinstance(value, str):\n",
    "        if '4' in value or '1' in value or 'positive' in value.lower():\n",
    "            return 1\n",
    "        elif '0' in value or 'negative' in value.lower():\n",
    "            return 0\n",
    "    return value  # Return as-is if not recognized\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(map_sentiment)\n",
    "\n",
    "# Remove any rows with NaN in sentiment\n",
    "df = df.dropna(subset=['sentiment'])\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "\n",
    "print(f\"\\nAfter processing:\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Positive samples (1): {(df['sentiment'] == 1).sum()}\")\n",
    "print(f\"Negative samples (0): {(df['sentiment'] == 0).sum()}\")\n",
    "print(f\"\\nSample texts:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"Row {i}: '{df['text'].iloc[i][:80]}...' -> Sentiment: {df['sentiment'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f36e6bb9-e1e6-43bd-8e6b-5798e96f59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text...\n",
      "\n",
      "Preprocessed 100000 samples\n",
      "\n",
      "Text preprocessing examples:\n",
      "\n",
      "Example 1 (Sentiment: Negative):\n",
      "  Original: @chrishasboobs AHHH I HOPE YOUR OK!!! ...\n",
      "  Cleaned:  ahhh hope ok...\n",
      "\n",
      "Example 2 (Sentiment: Negative):\n",
      "  Original: @misstoriblack cool , i have no tweet apps  for my razr 2...\n",
      "  Cleaned:  cool tweet apps razr 2...\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning regex patterns\n",
    "text_cleaning_re = r'@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9\\s]+'\n",
    "\n",
    "def preprocess_text(text, stem=False):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase and remove special characters/links\n",
    "    text = re.sub(text_cleaning_re, ' ', text.lower()).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to a sample for faster training\n",
    "print(\"Preprocessing text...\")\n",
    "sample_size = min(100000, len(df))  # Smaller sample for faster training\n",
    "df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(lambda x: preprocess_text(x, stem=False))\n",
    "\n",
    "print(f\"\\nPreprocessed {sample_size} samples\")\n",
    "print(\"\\nText preprocessing examples:\")\n",
    "for i in range(min(2, len(df_sample))):\n",
    "    print(f\"\\nExample {i+1} (Sentiment: {'Positive' if df_sample['sentiment'].iloc[i] == 1 else 'Negative'}):\")\n",
    "    print(f\"  Original: {df_sample['text'].iloc[i][:80]}...\")\n",
    "    print(f\"  Cleaned:  {df_sample['cleaned_text'].iloc[i][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f73e0b96-e555-4b15-817a-d94b9a648aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 70000\n",
      "Validation set size: 10000\n",
      "Test set size: 20000\n",
      "\n",
      "Class Distribution:\n",
      "Training:    sentiment\n",
      "0    34960\n",
      "1    35040\n",
      "Name: count, dtype: int64\n",
      "Validation:  sentiment\n",
      "0    4994\n",
      "1    5006\n",
      "Name: count, dtype: int64\n",
      "Test:        sentiment\n",
      "0     9989\n",
      "1    10011\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Creating tokenizer...\n",
      "\n",
      "Shapes:\n",
      "x_train: (70000, 30), y_train: (70000, 1)\n",
      "x_val:   (10000, 30), y_val: (10000, 1)\n",
      "x_test:  (20000, 30), y_test: (20000, 1)\n",
      "\n",
      "Vocabulary size: 43627\n",
      "Number of unique words: 43626\n",
      "\n",
      "Sample word mappings:\n",
      "  '<OOV>' -> 1\n",
      "  'good' -> 2\n",
      "  'day' -> 3\n",
      "  'get' -> 4\n",
      "  'like' -> 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "MAX_WORDS = 50000  # Reduced for faster training\n",
    "MAX_SEQ_LENGTH = 30\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(\n",
    "    df_sample, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    shuffle=True,\n",
    "    stratify=df_sample['sentiment']\n",
    ")\n",
    "\n",
    "# Further split train into train and validation\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=VALIDATION_SIZE/(1-TEST_SIZE),\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True,\n",
    "    stratify=train_df['sentiment']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(\"Training:   \", train_df['sentiment'].value_counts().sort_index())\n",
    "print(\"Validation: \", val_df['sentiment'].value_counts().sort_index())\n",
    "print(\"Test:       \", test_df['sentiment'].value_counts().sort_index())\n",
    "\n",
    "# Tokenization\n",
    "print(\"\\nCreating tokenizer...\")\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=MAX_WORDS,\n",
    "    oov_token=\"<OOV>\",\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    ")\n",
    "tokenizer.fit_on_texts(train_df['cleaned_text'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "def texts_to_sequences(data_df):\n",
    "    sequences = tokenizer.texts_to_sequences(data_df['cleaned_text'])\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences,\n",
    "        maxlen=MAX_SEQ_LENGTH,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    return padded\n",
    "\n",
    "x_train = texts_to_sequences(train_df)\n",
    "x_val = texts_to_sequences(val_df)\n",
    "x_test = texts_to_sequences(test_df)\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df['sentiment'].values.reshape(-1, 1)\n",
    "y_val = val_df['sentiment'].values.reshape(-1, 1)\n",
    "y_test = test_df['sentiment'].values.reshape(-1, 1)\n",
    "\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"x_val:   {x_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"x_test:  {x_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Vocabulary info\n",
    "vocab_size = min(MAX_WORDS, len(tokenizer.word_index)) + 1\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Number of unique words: {len(tokenizer.word_index)}\")\n",
    "print(f\"\\nSample word mappings:\")\n",
    "for word, idx in list(tokenizer.word_index.items())[:5]:\n",
    "    print(f\"  '{word}' -> {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14b78b66-3e59-40a4-843c-ee1dba0cefdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings from: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets/glove.6B.300d.txt\n",
      "Loaded 400000 word vectors\n",
      "Embedding dimension: 300\n",
      "\n",
      "Embedding matrix created. Shape: (43627, 300)\n",
      "Words with GloVe embeddings: 28945/43626\n",
      "Coverage: 66.35%\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings():\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from Datasets folder\n",
    "    \"\"\"\n",
    "    # Look for GloVe file\n",
    "    glove_files = [f for f in os.listdir(DATASETS_DIR) if 'glove' in f.lower()]\n",
    "    \n",
    "    if not glove_files:\n",
    "        print(\"No GloVe file found. Using random embeddings.\")\n",
    "        return None\n",
    "    \n",
    "    glove_path = os.path.join(DATASETS_DIR, glove_files[0])\n",
    "    print(f\"Loading GloVe embeddings from: {glove_path}\")\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                if len(values) > 1:\n",
    "                    word = values[0]\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = coefs\n",
    "        \n",
    "        print(f\"Loaded {len(embeddings_index)} word vectors\")\n",
    "        \n",
    "        # Check embedding dimension\n",
    "        sample_vector = next(iter(embeddings_index.values()))\n",
    "        embedding_dim = len(sample_vector)\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "        \n",
    "        return embeddings_index, embedding_dim\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GloVe: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load embeddings\n",
    "glove_result = load_glove_embeddings()\n",
    "if glove_result:\n",
    "    embeddings_index, EMBEDDING_DIM = glove_result\n",
    "else:\n",
    "    print(\"Using random embeddings\")\n",
    "    EMBEDDING_DIM = 300\n",
    "    embeddings_index = None\n",
    "\n",
    "# Create embedding matrix\n",
    "if embeddings_index and EMBEDDING_DIM:\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    \n",
    "    loaded_count = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i < MAX_WORDS:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                loaded_count += 1\n",
    "    \n",
    "    print(f\"\\nEmbedding matrix created. Shape: {embedding_matrix.shape}\")\n",
    "    print(f\"Words with GloVe embeddings: {loaded_count}/{min(MAX_WORDS, len(tokenizer.word_index))}\")\n",
    "    print(f\"Coverage: {loaded_count/min(MAX_WORDS, len(tokenizer.word_index)):.2%}\")\n",
    "else:\n",
    "    print(\"Creating random embedding matrix...\")\n",
    "    embedding_matrix = np.random.normal(size=(vocab_size, EMBEDDING_DIM))\n",
    "    print(f\"Random embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d9bad9a-b1c0-455e-b86c-321a0b474092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    \"\"\"Build RNN model\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_SEQ_LENGTH,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False  # Don't train embeddings if using GloVe\n",
    "        ),\n",
    "        layers.SimpleRNN(64, dropout=0.3, return_sequences=False),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"RNN Model Summary:\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def build_lstm_model():\n",
    "    \"\"\"Build LSTM model\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_SEQ_LENGTH,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False\n",
    "        ),\n",
    "        layers.LSTM(64, dropout=0.3, return_sequences=False),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM Model Summary:\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def build_gru_model():\n",
    "    \"\"\"Build GRU model\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_SEQ_LENGTH,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False\n",
    "        ),\n",
    "        layers.GRU(64, dropout=0.3, return_sequences=False),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"GRU Model Summary:\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "784d1da6-258f-44be-9715-cf8365d37332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_val, y_val, model_name, epochs=5, batch_size=64):\n",
    "    \"\"\"\n",
    "    Train a model and return history\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Simple callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title(f'{model_name} - Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title(f'{model_name} - Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e805a26-52e2-484b-a11b-dcbc32c45cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Store models and histories\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "# --- Train RNN Model (Level 1) ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEVEL 1: TRAINING RNN MODEL\")\n",
    "print(\"=\"*70)\n",
    "rnn_model = build_rnn_model()\n",
    "rnn_history = train_model(rnn_model, x_train, y_train, x_val, y_val, \"RNN\", EPOCHS, BATCH_SIZE)\n",
    "plot_training_history(rnn_history, \"RNN\")\n",
    "models['RNN'] = rnn_model\n",
    "histories['RNN'] = rnn_history\n",
    "\n",
    "# --- Train LSTM Model (Level 2) ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEVEL 2: TRAINING LSTM MODEL\")\n",
    "print(\"=\"*70)\n",
    "lstm_model = build_lstm_model()\n",
    "lstm_history = train_model(lstm_model, x_train, y_train, x_val, y_val, \"LSTM\", EPOCHS, BATCH_SIZE)\n",
    "plot_training_history(lstm_history, \"LSTM\")\n",
    "models['LSTM'] = lstm_model\n",
    "histories['LSTM'] = lstm_history\n",
    "\n",
    "# --- Train GRU Model (Level 3) ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEVEL 3: TRAINING GRU MODEL\")\n",
    "print(\"=\"*70)\n",
    "gru_model = build_gru_model()\n",
    "gru_history = train_model(gru_model, x_train, y_train, x_val, y_val, \"GRU\", EPOCHS, BATCH_SIZE)\n",
    "plot_training_history(gru_history, \"GRU\")\n",
    "models['GRU'] = gru_model\n",
    "histories['GRU'] = gru_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131dbc9-4b62-4657-990f-5cde3531a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, x_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(x_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    return accuracy, f1, y_pred_proba\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    accuracy, f1, pred_proba = evaluate_model(model, x_test, y_test, model_name)\n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c827510-d701-4c8a-af9e-1e085e611ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(model, x_test, y_test, test_df, model_name):\n",
    "    \"\"\"Analyze errors made by the model\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(x_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Find errors\n",
    "    errors = np.where(y_pred.flatten() != y_test.flatten())[0]\n",
    "    \n",
    "    print(f\"\\n{model_name} Error Analysis:\")\n",
    "    print(f\"Total test samples: {len(y_test)}\")\n",
    "    print(f\"Errors: {len(errors)} ({len(errors)/len(y_test):.2%})\")\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        # Show some error examples\n",
    "        print(\"\\nSample errors:\")\n",
    "        for i in errors[:3]:  # Show first 3 errors\n",
    "            original_text = test_df['text'].iloc[i]\n",
    "            true_label = \"Positive\" if y_test[i][0] == 1 else \"Negative\"\n",
    "            pred_label = \"Positive\" if y_pred[i][0] == 1 else \"Negative\"\n",
    "            confidence = y_pred_proba[i][0] if pred_label == \"Positive\" else 1 - y_pred_proba[i][0]\n",
    "            \n",
    "            print(f\"\\nError {i}:\")\n",
    "            print(f\"  Text: '{original_text[:100]}...'\")\n",
    "            print(f\"  True: {true_label}\")\n",
    "            print(f\"  Pred: {pred_label} (confidence: {confidence:.3f})\")\n",
    "\n",
    "# Perform error analysis for each model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    error_analysis(model, x_test, y_test, test_df, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354f307-78af-4ac4-9ffe-1f03c3cc3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_live(text, models_dict, tokenizer):\n",
    "    \"\"\"Predict sentiment for live text input\"\"\"\n",
    "    # Preprocess text\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequence,\n",
    "        maxlen=MAX_SEQ_LENGTH,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    \n",
    "    predictions = {}\n",
    "    for model_name, model in models_dict.items():\n",
    "        pred = model.predict(padded, verbose=0)[0][0]\n",
    "        sentiment = \"positive\" if pred > 0.5 else \"negative\"\n",
    "        score = float(pred)\n",
    "        \n",
    "        predictions[model_name] = {\n",
    "            'sentiment': sentiment,\n",
    "            'score': score\n",
    "        }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test with example sentences\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LIVE PREDICTION DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_sentences = [\n",
    "    \"I like reading.\",\n",
    "    \"This movie was terrible and boring.\",\n",
    "    \"Great service, highly recommended!\",\n",
    "    \"I'm not happy with the product quality.\",\n",
    "    \"Amazing experience, will come again!\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nInput: '{sentence}'\")\n",
    "    predictions = predict_sentiment_live(sentence, models, tokenizer)\n",
    "    \n",
    "    for model_name, pred in predictions.items():\n",
    "        print(f\"  {model_name}: {pred['sentiment']}, score: {pred['score']:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b898dc2-1537-4efa-b826-78112faf5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'saved_models')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "print(\"\\nSaving models...\")\n",
    "for model_name, model in models.items():\n",
    "    model_path = os.path.join(MODELS_DIR, f'{model_name.lower()}_model.h5')\n",
    "    model.save(model_path)\n",
    "    print(f\"  Saved {model_name} to {model_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "tokenizer_path = os.path.join(MODELS_DIR, 'tokenizer.pickle')\n",
    "with open(tokenizer_path, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"  Saved tokenizer to {tokenizer_path}\")\n",
    "\n",
    "# Save preprocessing parameters\n",
    "params = {\n",
    "    'MAX_SEQ_LENGTH': MAX_SEQ_LENGTH,\n",
    "    'MAX_WORDS': MAX_WORDS,\n",
    "    'EMBEDDING_DIM': EMBEDDING_DIM\n",
    "}\n",
    "import json\n",
    "params_path = os.path.join(MODELS_DIR, 'params.json')\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(params, f)\n",
    "print(f\"  Saved parameters to {params_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f542f-8677-46e5-aa52-62e970ba8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple text-based UI for demonstration\n",
    "ui_code = '''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "class SimpleSentimentUI:\n",
    "    def __init__(self):\n",
    "        # Load models and tokenizer\n",
    "        self.load_models()\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load saved models\"\"\"\n",
    "        self.models_dir = 'saved_models'\n",
    "        \n",
    "        # Load tokenizer\n",
    "        with open(os.path.join(self.models_dir, 'tokenizer.pickle'), 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        \n",
    "        # Load parameters\n",
    "        with open(os.path.join(self.models_dir, 'params.json'), 'r') as f:\n",
    "            params = json.load(f)\n",
    "            self.max_seq_length = params['MAX_SEQ_LENGTH']\n",
    "        \n",
    "        # Load models\n",
    "        self.models = {}\n",
    "        for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "            model_path = os.path.join(self.models_dir, f'{model_name.lower()}_model.h5')\n",
    "            if os.path.exists(model_path):\n",
    "                self.models[model_name] = tf.keras.models.load_model(model_path)\n",
    "                print(f\"Loaded {model_name} model\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.models)} models\")\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Simple text preprocessing\"\"\"\n",
    "        text = re.sub(r'[^A-Za-z0-9\\s]+', ' ', str(text).lower()).strip()\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Predict sentiment for given text\"\"\"\n",
    "        # Preprocess\n",
    "        cleaned_text = self.preprocess_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        sequence = self.tokenizer.texts_to_sequences([cleaned_text])\n",
    "        padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequence,\n",
    "            maxlen=self.max_seq_length,\n",
    "            padding='post',\n",
    "            truncating='post'\n",
    "        )\n",
    "        \n",
    "        predictions = {}\n",
    "        for model_name, model in self.models.items():\n",
    "            pred = model.predict(padded, verbose=0)[0][0]\n",
    "            sentiment = \"positive\" if pred > 0.5 else \"negative\"\n",
    "            predictions[model_name] = {\n",
    "                'sentiment': sentiment,\n",
    "                'score': float(pred)\n",
    "            }\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the interactive UI\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"SENTIMENT ANALYSIS DEMO\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        while True:\n",
    "            text = input(\"\\nEnter text to analyze: \").strip()\n",
    "            \n",
    "            if text.lower() == 'quit':\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nAnalyzing: '{text}'\")\n",
    "            predictions = self.predict(text)\n",
    "            \n",
    "            for model_name, pred in predictions.items():\n",
    "                print(f\"{model_name}: {pred['sentiment']}, score: {pred['score']:.8f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ui = SimpleSentimentUI()\n",
    "    ui.run()\n",
    "'''\n",
    "\n",
    "# Save UI script\n",
    "ui_file = os.path.join(BASE_DIR, 'sentiment_demo.py')\n",
    "with open(ui_file, 'w') as f:\n",
    "    f.write(ui_code)\n",
    "\n",
    "print(f\"\\nSimple UI script saved to: {ui_file}\")\n",
    "print(\"\\nTo run the demo:\")\n",
    "print(f\"python {ui_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
