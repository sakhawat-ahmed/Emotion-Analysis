{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241cd187-9a9e-47bb-9b61-a14f853918c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 22:06:39.763578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LEVEL 2: LSTM SENTIMENT ANALYSIS\n",
      "======================================================================\n",
      "Includes RNN from Level 1 and adds LSTM for comparison\n",
      "Working directory: /home/sakhawat/workspace/Python/ml/Emotion-Analysis\n",
      "Datasets directory: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets\n",
      "\n",
      "==================================================\n",
      "LOADING DATASET\n",
      "==================================================\n",
      "‚úÖ Found dataset: /home/sakhawat/workspace/Python/ml/Emotion-Analysis/Datasets/training.1600000.processed.noemoticon.csv\n",
      "‚ùå Error loading dataset: 'utf-8' codec can't decode bytes in position 7970-7971: invalid continuation byte\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 7970-7971: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Found dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Load dataset as per requirements\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Dataset loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Reset headers for the dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:252\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[1;32m    249\u001b[0m     Index \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, Sequence[Hashable] \u001b[38;5;241m|\u001b[39m MultiIndex, Mapping[Hashable, ArrayLike]\n\u001b[1;32m    250\u001b[0m ]:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(rows)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:1140\u001b[0m, in \u001b[0;36mPythonParser._get_lines\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     next_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_iter_line(row_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m rows \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1141\u001b[0m     rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:805\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(line, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 7970-7971: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LEVEL 2: COMPLETE LSTM SENTIMENT ANALYSIS\n",
    "# Using EXACT preprocessing and split from requirements\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LEVEL 2: LSTM SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Includes RNN from Level 1 and adds LSTM for comparison\")\n",
    "\n",
    "# ======================\n",
    "# 1. SETUP\n",
    "# ======================\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASETS_DIR = os.path.join(BASE_DIR, 'Datasets')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'saved_models')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {BASE_DIR}\")\n",
    "print(f\"Datasets directory: {DATASETS_DIR}\")\n",
    "\n",
    "# ======================\n",
    "# 2. LOAD DATASET\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Look for dataset file\n",
    "dataset_files = [f for f in os.listdir(DATASETS_DIR) \n",
    "                if 'training' in f.lower() and f.endswith('.csv')]\n",
    "\n",
    "if not dataset_files:\n",
    "    print(\"‚ùå ERROR: Dataset file not found in Datasets folder!\")\n",
    "    print(\"Please make sure 'training.1600000.processed.noemoticon.csv' is in Datasets folder\")\n",
    "    raise FileNotFoundError(\"Dataset file not found\")\n",
    "\n",
    "file_path = os.path.join(DATASETS_DIR, dataset_files[0])\n",
    "print(f\"‚úÖ Found dataset: {file_path}\")\n",
    "\n",
    "try:\n",
    "    # Load dataset as per requirements\n",
    "    dataset = pd.read_csv(file_path, engine=\"python\", header=None)\n",
    "    print(\"‚úÖ Dataset loaded successfully\")\n",
    "    \n",
    "    # Reset headers for the dataset\n",
    "    dataset.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "    \n",
    "    # Drop useless columns vertically\n",
    "    df = dataset.drop(['id', 'date', 'query', 'user_id'], axis=1)\n",
    "    \n",
    "    # Check Label Categories\n",
    "    print(\"\\nüìä Label distribution:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# ======================\n",
    "# 3. TEXT PREPROCESSING (EXACT from requirements)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# EXACT preprocessing function from requirements\n",
    "text_cleaning_re = '@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+'\n",
    "\n",
    "def preprocessing(text, stem=False):\n",
    "    \"\"\"EXACT preprocessing function from requirements\"\"\"\n",
    "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        tokens.append(token)  # Save word directly (no stopwords removal as in requirements)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean each row of text in the text column of the dataset\n",
    "df.text = df.text.apply(lambda x: preprocessing(x))\n",
    "\n",
    "print(\"‚úÖ Text preprocessing completed\")\n",
    "\n",
    "# ======================\n",
    "# 4. TRAIN-TEST SPLIT (EXACT from requirements)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "MAX_WORDS = 100000  # Maximum vocabulary size 100,000\n",
    "MAX_SEQ_LENGTH = 30  # Maximum sequence length 30\n",
    "\n",
    "# Split train and test sets (EXACT from requirements)\n",
    "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=666, shuffle=True)\n",
    "\n",
    "print(f\"üìö Training set size: {len(train_dataset):,}\")\n",
    "print(f\"üß™ Test set size: {len(test_dataset):,}\")\n",
    "\n",
    "# ======================\n",
    "# 5. TOKENIZATION (EXACT from requirements)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOKENIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Tokenization (EXACT from requirements)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(train_dataset.text)\n",
    "\n",
    "# Each word corresponds to an index\n",
    "word_index = tokenizer.word_index\n",
    "# Training set vocabulary size\n",
    "vocab_size = len(word_index) + 1\n",
    "print(f\"üìñ Vocabulary size: {vocab_size:,}\")\n",
    "\n",
    "# Fix the length of each text\n",
    "# Convert words to sequences (EXACT from requirements)\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(train_dataset.text),\n",
    "    maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(test_dataset.text),\n",
    "    maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "# Perform LabelEncoding on label categories, encode categories into continuous numbers\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(train_dataset.sentiment.tolist())\n",
    "y_test = encoder.fit_transform(test_dataset.sentiment.tolist())\n",
    "y_train = y_train.reshape(-1, 1)  # Reshape\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"\\nüìê Data shapes (EXACT from requirements):\")\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"x_test:  {x_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# For validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"x_val:   {x_val.shape}, y_val: {y_val.shape}\")\n",
    "\n",
    "# ======================\n",
    "# 6. BUILD RNN MODEL (Level 1)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REBUILDING RNN MODEL (Level 1)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def build_rnn_model():\n",
    "    \"\"\"Build RNN model for Level 1\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=MAX_WORDS,\n",
    "            output_dim=128,\n",
    "            input_length=MAX_SEQ_LENGTH,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        layers.SimpleRNN(128, dropout=0.2, return_sequences=False),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train RNN model\n",
    "print(\"üöÄ Training RNN model...\")\n",
    "rnn_model = build_rnn_model()\n",
    "rnn_history = rnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RNN training completed!\")\n",
    "\n",
    "# ======================\n",
    "# 7. BUILD LSTM MODEL (Level 2)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BUILDING LSTM MODEL (Level 2)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def build_lstm_model():\n",
    "    \"\"\"Build LSTM model for Level 2\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Embedding layer\n",
    "        layers.Embedding(\n",
    "            input_dim=MAX_WORDS,\n",
    "            output_dim=128,\n",
    "            input_length=MAX_SEQ_LENGTH,\n",
    "            mask_zero=True\n",
    "        ),\n",
    "        \n",
    "        # LSTM layer\n",
    "        layers.LSTM(\n",
    "            128,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            return_sequences=False\n",
    "        ),\n",
    "        \n",
    "        # Dense layer\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = build_lstm_model()\n",
    "print(\"LSTM Model Summary:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# ======================\n",
    "# 8. TRAIN LSTM MODEL\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING LSTM MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"üöÄ Training LSTM model...\")\n",
    "lstm_history = lstm_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LSTM training completed!\")\n",
    "\n",
    "# ======================\n",
    "# 9. PLOT TRAINING CURVES FOR BOTH MODELS\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING CURVES COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# RNN Loss\n",
    "axes[0, 0].plot(rnn_history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(rnn_history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_title('RNN - Loss Curve', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RNN Accuracy\n",
    "axes[0, 1].plot(rnn_history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(rnn_history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('RNN - Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# LSTM Loss\n",
    "axes[1, 0].plot(lstm_history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[1, 0].plot(lstm_history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1, 0].set_title('LSTM - Loss Curve', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# LSTM Accuracy\n",
    "axes[1, 1].plot(lstm_history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1, 1].plot(lstm_history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1, 1].set_title('LSTM - Accuracy Curve', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================\n",
    "# 10. EVALUATE BOTH MODELS\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION & COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_model(model, model_name):\n",
    "    \"\"\"Evaluate a model and return metrics\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(x_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} Evaluation:\")\n",
    "    print(f\"  Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Test F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title(f'{model_name} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "# Evaluate RNN\n",
    "rnn_accuracy, rnn_f1 = evaluate_model(rnn_model, \"RNN\")\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_accuracy, lstm_f1 = evaluate_model(lstm_model, \"LSTM\")\n",
    "\n",
    "# ======================\n",
    "# 11. MODEL COMPARISON\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RNN vs LSTM COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüìà Performance Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Metric':<15} {'RNN':<10} {'LSTM':<10} {'Difference':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Accuracy':<15} {rnn_accuracy:.4f}     {lstm_accuracy:.4f}     {lstm_accuracy - rnn_accuracy:+.4f}\")\n",
    "print(f\"{'F1-Score':<15} {rnn_f1:.4f}     {lstm_f1:.4f}     {lstm_f1 - rnn_f1:+.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nüîç Architectural Comparison:\")\n",
    "print(\"1. RNN (SimpleRNN):\")\n",
    "print(\"   - Simple recurrent connections\")\n",
    "print(\"   - Prone to vanishing gradient problem\")\n",
    "print(\"   - Less parameters\")\n",
    "print(\"   - Faster to train\")\n",
    "\n",
    "print(\"\\n2. LSTM (Long Short-Term Memory):\")\n",
    "print(\"   - Has memory cells and gates (input, forget, output)\")\n",
    "print(\"   - Solves vanishing gradient problem\")\n",
    "print(\"   - Better at capturing long-term dependencies\")\n",
    "print(\"   - More parameters, slower to train\")\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "if lstm_accuracy > rnn_accuracy:\n",
    "    print(\"‚úÖ LSTM performs better than RNN on this sentiment analysis task.\")\n",
    "    print(\"   This is expected due to LSTM's ability to capture long-range dependencies.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  RNN performs similarly or better than LSTM.\")\n",
    "    print(\"   This could be because sentiment analysis often relies on short-range dependencies.\")\n",
    "\n",
    "# ======================\n",
    "# 12. LIVE PREDICTION DEMO\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LIVE PREDICTION DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def predict_single_text(text):\n",
    "    \"\"\"Predict sentiment for a single text using both models\"\"\"\n",
    "    # Preprocess using the EXACT preprocessing function\n",
    "    cleaned_text = preprocessing(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequence,\n",
    "        maxlen=MAX_SEQ_LENGTH,\n",
    "        padding='post',\n",
    "        truncating='post'\n",
    "    )\n",
    "    \n",
    "    # Predict with RNN\n",
    "    rnn_pred = rnn_model.predict(padded, verbose=0)[0][0]\n",
    "    rnn_sentiment = \"positive\" if rnn_pred > 0.5 else \"negative\"\n",
    "    \n",
    "    # Predict with LSTM\n",
    "    lstm_pred = lstm_model.predict(padded, verbose=0)[0][0]\n",
    "    lstm_sentiment = \"positive\" if lstm_pred > 0.5 else \"negative\"\n",
    "    \n",
    "    return {\n",
    "        'RNN': {'sentiment': rnn_sentiment, 'score': float(rnn_pred)},\n",
    "        'LSTM': {'sentiment': lstm_sentiment, 'score': float(lstm_pred)}\n",
    "    }\n",
    "\n",
    "# Test with the teacher's example\n",
    "test_text = \"I like reading.\"\n",
    "print(f\"\\nüß™ Testing with teacher's example: '{test_text}'\")\n",
    "predictions = predict_single_text(test_text)\n",
    "\n",
    "for model_name, pred in predictions.items():\n",
    "    print(f\"{model_name} prediction result: {pred['sentiment']}, score: {pred['score']:.8f}\")\n",
    "\n",
    "# ======================\n",
    "# 13. SAVE MODELS\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save RNN model\n",
    "rnn_model_path = os.path.join(MODELS_DIR, 'rnn_model.h5')\n",
    "rnn_model.save(rnn_model_path)\n",
    "print(f\"‚úÖ RNN model saved to: {rnn_model_path}\")\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_model_path = os.path.join(MODELS_DIR, 'lstm_model.h5')\n",
    "lstm_model.save(lstm_model_path)\n",
    "print(f\"‚úÖ LSTM model saved to: {lstm_model_path}\")\n",
    "\n",
    "# Save tokenizer (overwrite if exists)\n",
    "import pickle\n",
    "tokenizer_path = os.path.join(MODELS_DIR, 'tokenizer.pickle')\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"‚úÖ Tokenizer saved to: {tokenizer_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEVEL 2 COMPLETED: RNN & LSTM MODELS TRAINED AND SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä RNN Accuracy: {rnn_accuracy:.4f}, F1: {rnn_f1:.4f}\")\n",
    "print(f\"üìä LSTM Accuracy: {lstm_accuracy:.4f}, F1: {lstm_f1:.4f}\")\n",
    "print(f\"üìÅ Models saved in: {MODELS_DIR}\")\n",
    "print(\"\\nFor teacher's demonstration with 'I like reading.':\")\n",
    "print(\"Expected output: RNN prediction result: positive, score: 0.61676633\")\n",
    "print(\"Expected output: LSTM prediction result: positive, score: 0.7692368\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a1439-6ebc-4089-a06d-b7af7adc8895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
